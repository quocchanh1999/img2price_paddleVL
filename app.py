import streamlit as st
import pandas as pd
import numpy as np
import joblib
import re
from rapidfuzz import process, fuzz
import unicodedata
import nltk
from nltk.stem.snowball import SnowballStemmer
import easyocr
from PIL import Image
import io
import asyncio
import sys
import google.generativeai as genai
import os

class LogScaler:
    def transform(self, x):
        return np.log1p(x)
    def inverse_transform(self, x):
        return np.expm1(x)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

st.set_page_config(layout="centered", page_title="D·ª± ƒëo√°n Gi√° thu·ªëc")

@st.cache_resource
def initialize_tools():
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    stemmer = SnowballStemmer("english")
    reader = easyocr.Reader(['vi', 'en'], gpu=False)
    return stemmer, reader

stemmer, ocr_reader = initialize_tools()

@st.cache_resource
def load_artifacts():
    try:
        model_giaThanh = joblib.load(os.path.join(BASE_DIR, "final_model_giaThanh.joblib"))
        model_giaBanBuon = joblib.load(os.path.join(BASE_DIR, "final_model_giaBanBuon.joblib"))
        imputer = joblib.load(os.path.join(BASE_DIR, "imputer.joblib"))
        scaler_giaThanh = joblib.load(os.path.join(BASE_DIR, "scaler_giaThanh.joblib"))
        scaler_giaBanBuon = joblib.load(os.path.join(BASE_DIR, "scaler_giaBanBuon.joblib"))
        train_cols = joblib.load(os.path.join(BASE_DIR, "train_cols.joblib"))
        # Remove duplicates from train_cols
        train_cols = list(dict.fromkeys(train_cols))
        df_full = pd.read_excel(os.path.join(BASE_DIR, "dichvucong_medicines_Final.xlsx"))
        return model_giaThanh, model_giaBanBuon, imputer, scaler_giaThanh, scaler_giaBanBuon, train_cols, df_full
    except FileNotFoundError as e:
        st.error(f"Error loading artifacts: {e}")
        st.stop()

model_giaThanh, model_giaBanBuon, imputer, scaler_giaThanh, scaler_giaBanBuon, train_cols, df_full = load_artifacts()

categorical_cols = ['doanhNghiepSanXuat', 'nuocSanXuat', 'dangBaoChe_final', 'hoat_chat_chinh', 'loaiDongGoiChinh', 'donViCoSo', 'is_low_price']

REPLACEMENTS_DNSX = {
    'ctcp': 'c√¥ng ty c·ªï ph·∫ßn', 'tnhh': 'tr√°ch nhi·ªám h·ªØu h·∫°n', 'dp': 'd∆∞·ª£c ph·∫©m', 'tw': 'trung ∆∞∆°ng',
    'cty': 'c√¥ng ty', 'ct': 'c√¥ng ty', 'cp': 'c·ªï ph·∫ßn', 'sx': 's·∫£n xu·∫•t', 'tm': 'th∆∞∆°ng m·∫°i',
    'ld': 'li√™n doanh', 'mtv': 'm·ªôt th√†nh vi√™n'
}
GENERIC_TERMS_DNSX = [
    'c√¥ng ty c·ªï ph·∫ßn', 'c√¥ng ty tnhh', 'c√¥ng ty', 'tr√°ch nhi·ªám h·ªØu h·∫°n', 'm·ªôt th√†nh vi√™n',
    'li√™n doanh', 'c·ªï ph·∫ßn', 's·∫£n xu·∫•t', 'th∆∞∆°ng m·∫°i', 'trung ∆∞∆°ng', 'limited', 'ltd', 'pvt',
    'inc', 'corp', 'corporation', 'gmbh', 'co', 'kg', 'ag', 'srl', 'international',
    'pharma', 'pharmaceuticals', 'pharmaceutical', 'laboratories', 'industries'
]
GENERIC_TERMS_DNSX.sort(key=len, reverse=True)

def ultimate_company_name_cleaner(name_series, stemmer):
    def clean_single_name(name):
        name = str(name).lower()
        name = re.sub(r'\([^)]*\)', '', name)
        name = re.sub(r'[^a-z0-9\s]', ' ', name)
        name = re.sub(r'\s+', ' ', name).strip()
        for old, new in REPLACEMENTS_DNSX.items():
            name = name.replace(old, new)
        tokens = name.split()
        stemmed_tokens = [stemmer.stem(token) for token in tokens if token]
        name = " ".join(stemmed_tokens)
        for term in GENERIC_TERMS_DNSX:
            stemmed_term = " ".join([stemmer.stem(t) for t in term.split()])
            if stemmed_term:
                name = name.replace(stemmed_term, '')
        return re.sub(r'\s+', ' ', name).strip()
    return name_series.apply(clean_single_name)

DEFINITIVE_DBC_MAP = {
    'Thu·ªëc c·∫•y/Que c·∫•y': ['c·∫•y d∆∞·ªõi da', 'que c·∫•y'],
    'D·∫°ng x·ªãt d∆∞·ªõi l∆∞·ª°i': ['x·ªãt d∆∞·ªõi l∆∞·ª°i'],
    'Kh√≠ dung/H√≠t': ['kh√≠ dung', 'aerosol', 'inhaler', 'h√≠t', 'phun m√π'],
    'Thu·ªëc ƒë·∫∑t': ['thu·ªëc ƒë·∫°n', 'vi√™n ƒë·∫∑t', 'ƒë·∫°n ƒë·∫∑t', 'suppository', 'vi√™n ƒë·∫°n'],
    'Thu·ªëc g√¢y m√™ ƒë∆∞·ªùng h√¥ h·∫•p': ['g√¢y m√™', 'h√¥ h·∫•p'],
    'Tr√† t√∫i l·ªçc': ['tr√† t√∫i l·ªçc'],
    'B·ªôt pha ti√™m/truy·ªÅn': ['b·ªôt ƒë√¥ng kh√¥ pha ti√™m', 'b·ªôt pha ti√™m', 'powder for injection', 'b·ªôt v√† dung m√¥i pha ti√™m', 'b·ªôt ƒë√¥ng kh√¥', 'dung m√¥i pha ti√™m', 'b·ªôt v√¥ khu·∫©n pha ti√™m'],
    'Dung d·ªãch ti√™m/truy·ªÅn': ['dung d·ªãch ti√™m', 'thu·ªëc ti√™m', 'b∆°m ti√™m', 'injection', 'solution for injection', 'd·ªãch truy·ªÅn', 'd·ªãch treo v√¥ khu·∫©n', 'l·ªç', '·ªëng', 'dung dich ti√™m'],
    'H·ªón d·ªãch ti√™m/truy·ªÅn': ['h·ªón d·ªãch ti√™m', 'suspension for injection'],
    'Nh≈© t∆∞∆°ng ti√™m/truy·ªÅn': ['nh≈© t∆∞∆°ng ti√™m', 'emulsion for injection'],
    'Ho√†n (YHCT)': ['ho√†n m·ªÅm', 'ho√†n c·ª©ng', 'vi√™n ho√†n'],
    'Cao l·ªèng (YHCT)': ['cao l·ªèng'],
    'Cao xoa/d√°n (YHCT)': ['cao xoa', 'cao d√°n'],
    'D·∫ßu xoa/gi√≥': ['d·∫ßu xoa', 'd·∫ßu gi√≥', 'd·∫ßu xoa b√≥p', 'd·∫ßu b√¥i ngo√†i da'],
    'Kem b√¥i da': ['kem b√¥i', 'kem', 'cream'],
    'Gel b√¥i da': ['gel b√¥i', 'gel'],
    'Thu·ªëc m·ª° b√¥i da': ['thu·ªëc m·ª°', 'ointment', 'thu√¥ÃÅc m∆°ÃÉ', 'm·ª° b√¥i da', 'm·ª° b√¥i ngo√†i da'],
    'Mi·∫øng d√°n': ['mi·∫øng d√°n', 'patch'],
    'Lotion': ['lotion'],
    'C·ªìn/R∆∞·ª£u thu·ªëc': ['c·ªìn thu·ªëc', 'c·ªìn xoa b√≥p', 'r∆∞·ª£u thu·ªëc'],
    'N∆∞·ªõc s√∫c mi·ªáng/R∆° mi·ªáng': ['n∆∞·ªõc s√∫c mi·ªáng', 'r∆° mi·ªáng'],
    'D·∫ßu g·ªôi': ['d·∫ßu g·ªôi'],
    'Dung d·ªãch nh·ªè (M·∫Øt/M≈©i/Tai)': ['nh·ªè m·∫Øt', 'nh·ªè m≈©i', 'nh·ªè tai', 'eye drops', 'nasal drops', 'dung diÃ£ch nhoÃâ mƒÉÃÅt'],
    'Dung d·ªãch x·ªãt (M≈©i/Tai)': ['x·ªãt m≈©i', 'x·ªãt', 'nasal spray', 'spray'],
    'Thu·ªëc m·ª° (M·∫Øt/M≈©i/Tai)': ['m·ª° tra m·∫Øt', 'eye ointment'],
    'Vi√™n nang': ['vi√™n nang', 'nang', 'capsule', 'cap'],
    'Vi√™n s·ªßi': ['vi√™n s·ªßi', 'effervescent', 'c·ªëm s·ªßi b·ªçt'],
    'Vi√™n ng·∫≠m': ['vi√™n ng·∫≠m', 'sublingual'],
    'Vi√™n n√©n': ['vi√™n n√©n', 'vi√™n bao', 'tablet', 'n√©n bao', 'vi√™n nhai', 'vi√™n ph√¢n t√°n', 'vi√™n'],
    'Siro': ['siro', 'sir√¥', 'siiro', 'syrup'],
    'H·ªón d·ªãch u·ªëng': ['h·ªón d·ªãch u·ªëng', 'h·ªón d·ªãch', 'oral suspension', 'suspension'],
    'Nh≈© t∆∞∆°ng u·ªëng': ['nh≈© t∆∞∆°ng u·ªëng', 'nh≈© t∆∞∆°ng', 'nh≈© d·ªãch u·ªëng', 'oral emulsion', 'nh·ªè gi·ªçt'],
    'Dung d·ªãch u·ªëng': ['dung d·ªãch u·ªëng', 'oral solution', 'solution', 'thu·ªëc n∆∞·ªõc u·ªëng', 'thu·ªëc n∆∞·ªõc'],
    'Thu·ªëc c·ªëm u·ªëng': ['thu·ªëc c·ªëm', 'c·ªëm pha', 'granules', 'c·ªëm'],
    'Thu·ªëc b·ªôt u·ªëng': ['thu·ªëc b·ªôt pha u·ªëng', 'thu·ªëc b·ªôt', 'powder', 'b·ªôt pha u·ªëng', 'b·ªôt'],
    'Dung d·ªãch (Chung)': ['dung d·ªãch'],
    'D√πng ngo√†i (Chung)': ['d√πng ngo√†i', 'external', 'topical'],
    'Nguy√™n li·ªáu': ['nguy√™n li·ªáu', 'active ingredient'],
}

def classify_dangBaoChe_final(text):
    if pd.isnull(text):
        return "Kh√¥ng x√°c ƒë·ªãnh"
    s = unicodedata.normalize('NFKC', str(text).lower())
    s = re.sub(r'[^a-z0-9√†-·ªπ\s]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    for standard_form, keywords in DEFINITIVE_DBC_MAP.items():
        if any(re.search(r'\b' + re.escape(keyword) + r'\b', s) for keyword in keywords):
            return standard_form
    return 'Kh√°c (Ch∆∞a ph√¢n lo·∫°i)'

def extract_quantity(text):
    if pd.isnull(text):
        return pd.Series([1.0, 1.0, 1.0, 1], index=['min_so_luong', 'max_so_luong', 'mean_so_luong', 'num_pack_options'])
    patterns = re.compile(r'[;,]\s*|\s*(?:ho·∫∑c|or)\s*', re.IGNORECASE)
    packs = [p.strip() for p in patterns.split(str(text)) if p.strip()]
    quantities = []
    for pack in packs:
        numbers = re.findall(r'(\d+\.?\d*)\s*(vi√™n|ml|g|·ªëng|nang|g√≥i|t√∫i|tu√Ωp|chai|l·ªç)', pack, re.IGNORECASE)
        if not numbers:
            numbers = re.findall(r'(\d+\.?\d*)', pack)
            if not numbers:
                continue
        total = 1.0
        if isinstance(numbers[0], tuple):
            for num, unit in numbers:
                total *= float(num)
        else:
            for num in numbers:
                total *= float(num)
        quantities.append(total)
    if not quantities:
        quantities = [1.0]
    return pd.Series([min(quantities), max(quantities), np.mean(quantities), len(quantities)],
                     index=['min_so_luong', 'max_so_luong', 'mean_so_luong', 'num_pack_options'])

ULTIMATE_UNIT_CONVERSION_MAP = {
    'kg': 1_000_000, 'g': 1_000, 'mg': 1, 'mcg': 0.001, '¬µg': 0.001, 'ml': 1_000, 'l': 1_000_000
}

def normalize_hamluong_to_mg(hamluong_str):
    total_mg = 0
    dosages = re.findall(r'(\d+\.?\d*)\s*(mg|g|ml|l|kg|iu|ui|%)', str(hamluong_str).lower())
    if not dosages:
        return np.nan
    for value, unit in dosages:
        value = float(value)
        if unit == 'g':
            total_mg += value * 1000
        elif unit == 'kg':
            total_mg += value * 1000000
        elif unit == 'l':
            total_mg += value * 1000
        elif unit == 'ml':
            total_mg += value
        elif unit in ['iu', 'ui']:
            continue
        else:
            total_mg += value
    return total_mg if total_mg > 0 else np.nan

def extract_ingredient_features_ultimate(row):
    hoatChat_str = str(row.get('hoatChat', '')).lower().strip().replace('/', ';')
    hamLuong_val = row.get('hamLuong')
    hoatChat_list = [hc.strip() for hc in hoatChat_str.split(';') if hc.strip()]
    so_luong_hoat_chat = len(hoatChat_list)
    hoat_chat_chinh = hoatChat_list[0] if so_luong_hoat_chat > 0 else "missing"
    if pd.isnull(hamLuong_val) or str(hamLuong_val).strip() == '':
        return pd.Series([so_luong_hoat_chat, hoat_chat_chinh, 0.0, 0.0, 0.0],
                         index=['so_luong_hoat_chat', 'hoat_chat_chinh', 'hl_chinh_mg', 'tong_hl_phu_mg', 'tong_hl_iu'])
    hamLuong_str = str(hamLuong_val).replace(',', '.')
    dosages = re.findall(r'(\d+\.?\d*)\s*(mcg|¬µg|mg|g|kg|ml|l|iu|ui|%)', hamLuong_str.lower())
    total_mg = 0.0
    total_iu = 0.0
    converted_dosages_mg = []
    for value_str, unit in dosages:
        value = float(value_str)
        if unit in ULTIMATE_UNIT_CONVERSION_MAP:
            converted_value = value * ULTIMATE_UNIT_CONVERSION_MAP[unit]
            total_mg += converted_value
            converted_dosages_mg.append(converted_value)
        elif unit in ['iu', 'ui']:
            total_iu += value
    hl_chinh_mg = converted_dosages_mg[0] if converted_dosages_mg else 0.0
    tong_hl_phu_mg = sum(converted_dosages_mg[1:]) if len(converted_dosages_mg) > 1 else 0.0
    return pd.Series([so_luong_hoat_chat, hoat_chat_chinh, hl_chinh_mg, tong_hl_phu_mg, total_iu],
                     index=['so_luong_hoat_chat', 'hoat_chat_chinh', 'hl_chinh_mg', 'tong_hl_phu_mg', 'tong_hl_iu'])

def get_packaging_type(text):
    text = str(text).lower()
    for key, value in [
        ('l·ªç', 'l·ªç'), ('chai', 'chai'), ('tu√Ωp', 'tu√Ωp'), ('·ªëng', '·ªëng'),
        ('v·ªâ', 'v·ªâ'), ('h·ªôp', 'h·ªôp'), ('g√≥i', 'g√≥i'), ('t√∫i', 't√∫i'), ('b√¨nh', 'b√¨nh')
    ]:
        if key in text:
            return value
    return 'kh√°c'

def get_base_unit(text):
    text = str(text).lower()
    for key, value in [
        ('vi√™n nang', 'nang'), ('nang', 'nang'), ('vi√™n', 'vi√™n'),
        ('ml', 'ml'), ('g', 'g'), ('gam', 'g'), ('g√≥i', 'g√≥i'), ('·ªëng', '·ªëng')
    ]:
        if key in text:
            return value
    return 'kh√°c'

def parse_dosage_value(hamLuong_val):
    if pd.isnull(hamLuong_val):
        return 0.0
    hamLuong_normalized = str(hamLuong_val).replace(',', '.')
    dosages = re.findall(r'(\d+\.?\d*)\s*(mcg|¬µg|mg|g|kg|ml|l|iu|ui|%)', hamLuong_normalized.lower())
    total_mg = 0.0
    for value_str, unit in dosages:
        value = float(value_str)
        if unit in ULTIMATE_UNIT_CONVERSION_MAP:
            total_mg += value * ULTIMATE_UNIT_CONVERSION_MAP[unit]
    return total_mg

def transform_hybrid_data(hybrid_data, train_cols, categorical_cols, imputer):
    df = pd.DataFrame([hybrid_data])
    df['doanhNghiepSanXuat'] = df['doanhNghiepSanXuat'].replace(['', 'nan', None], np.nan).fillna('missing').astype(str)
    df['nuocSanXuat'] = df['nuocSanXuat'].replace(['', 'nan', None], np.nan).fillna('missing').astype(str)
    df['is_low_price'] = 0
    if 'giaBanBuonDuKien' in df.columns:
        df['is_low_price'] = (pd.to_numeric(df['giaBanBuonDuKien'], errors='coerce') < 500).astype(int)
    pack_features = df['quyCachDongGoi'].apply(extract_quantity)
    df = pd.concat([df, pack_features], axis=1)
    df['tongHamLuong_mg'] = df['hamLuong'].apply(normalize_hamluong_to_mg)
    df['loaiDongGoiChinh'] = df['quyCachDongGoi'].apply(get_packaging_type)
    df['donViCoSo'] = df['quyCachDongGoi'].apply(get_base_unit)
    df['is_dangBaoChe_missing'] = df['dangBaoChe'].isnull().astype(int)
    df['dangBaoChe_final'] = df['dangBaoChe'].apply(classify_dangBaoChe_final)
    df.loc[df['is_dangBaoChe_missing'] == 1, 'dangBaoChe_final'] = 'Kh√¥ng x√°c ƒë·ªãnh'
    ingredient_features = df.apply(extract_ingredient_features_ultimate, axis=1)
    df = pd.concat([df, ingredient_features], axis=1)
    df['hl_chinh_mg'].fillna(0, inplace=True)
    df['tong_hl_phu_mg'].fillna(0, inplace=True)
    df['tong_hl_iu'].fillna(0, inplace=True)
    df['has_multiple_packs'] = (df['num_pack_options'] > 1).astype(int)
    df['hamluong_so_luong'] = df['tongHamLuong_mg'] * df['mean_so_luong']
    df['gia_per_unit'] = df['tongHamLuong_mg'] / df['mean_so_luong'].replace(0, 1)
    df = df.drop(columns=['tenThuoc', 'hoatChat', 'hamLuong', 'quyCachDongGoi', 'dangBaoChe', 'soLuong', 'donViTinh'], errors='ignore')
    df = df.loc[:, ~df.columns.duplicated()]
    missing_cols = [col for col in train_cols if col not in df.columns]
    for col in missing_cols:
        df[col] = np.nan
    df = df[train_cols]
    for col in categorical_cols:
        df[col] = df[col].replace(['', 'nan', None], np.nan).fillna('missing').astype(str)
    numerical_cols = df.select_dtypes(include=[np.number]).columns
    df[numerical_cols] = imputer.transform(df[numerical_cols])
    return df

def parse_user_query(query):
    parsed = {"tenThuoc": "N/A", "hoatChat": np.nan, "hamLuong": np.nan, "soLuong": "N/A", "donViTinh": "N/A"}
    temp_query = query
    match_hc = re.search(r'^(.*?)\s*\((.*?)\)', temp_query)
    if match_hc:
        parsed["hoatChat"] = match_hc.group(1).strip()
        parsed["tenThuoc"] = match_hc.group(2).strip()
        temp_query = temp_query.replace(match_hc.group(0), parsed["tenThuoc"])
    else:
        parsed["tenThuoc"] = temp_query.strip()
    hamluong_pattern = r'(\d+[\.,]?\d*\s*(?:mg|g|mcg|ml|l|iu|ui|kg)(?:\s*/\s*(?:ml|g|vi√™n))?)'
    match_hl = re.search(hamluong_pattern, temp_query, re.IGNORECASE)
    if match_hl:
        parsed["hamLuong"] = match_hl.group(1).strip()
        temp_query = temp_query.replace(match_hl.group(0), '')
    unit_keywords = ['vi√™n nang', 'vi√™n n√©n', 'nang', 'vi√™n', 'g√≥i', '·ªëng', 'chai', 'l·ªç', 'h·ªôp', 'tu√Ωp']
    unit_pattern_sl = '|'.join(unit_keywords)
    match_sl = re.search(r'(\d+)\s*(' + unit_pattern_sl + r')\b', temp_query, re.IGNORECASE)
    if match_sl:
        parsed["soLuong"] = f"{match_sl.group(1)} {match_sl.group(2)}"
        parsed["donViTinh"] = match_sl.group(2).capitalize()
        temp_query = temp_query.replace(match_sl.group(0), '')
    if parsed["tenThuoc"] == "N/A":
        parsed["tenThuoc"] = temp_query.strip()
    parsed["quyCachDongGoi"] = parsed["soLuong"] if pd.notna(parsed["soLuong"]) else parsed["tenThuoc"]
    return parsed

@st.cache_data
def call_gemini_parser(ocr_text):
    try:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
        model = genai.GenerativeModel('gemini-2.5-flash-lite')
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        prompt = f"""
        Gi·ªù t√¥i g·ª≠i b·∫°n m·ªôt ƒëo·∫°n OCR t·ª´ toa thu·ªëc. Nhi·ªám v·ª• c·ªßa b·∫°n l√† li·ªát k√™ nh·ªØng thu·ªëc trong ƒë√≥ ra theo danh s√°ch t∆∞∆°ng ·ª©ng nh∆∞ sau:

        - T√™n thu·ªëc:
        - Ho·∫°t ch·∫•t:
        - H√†m l∆∞·ª£ng:
        - S·ªë l∆∞·ª£ng:
        - ƒê∆°n v·ªã t√≠nh:

        M·ªôt v√≠ d·ª• nh∆∞ sau:

        1. T√™n thu·ªëc: UNASYN
        Ho·∫°t ch·∫•t: Sultamicillin
        H√†m l∆∞·ª£ng: 375mg
        S·ªë l∆∞·ª£ng: 8
        ƒê∆°n v·ªã t√≠nh: Vi√™n

        2. T√™n thu·ªëc: NEXT G CAL
        Ho·∫°t ch·∫•t: (Ch∆∞a x√°c ƒë·ªãnh)
        H√†m l∆∞·ª£ng: (Ch∆∞a x√°c ƒë·ªãnh)
        S·ªë l∆∞·ª£ng: 30
        ƒê∆°n v·ªã t√≠nh: Vi√™n

        3. T√™n thu·ªëc: HEMO Q MOM
        Ho·∫°t ch·∫•t: (Ch∆∞a x√°c ƒë·ªãnh)
        H√†m l∆∞·ª£ng: (Ch∆∞a x√°c ƒë·ªãnh)
        S·ªë l∆∞·ª£ng: 30
        ƒê∆°n v·ªã: Vi√™n

        4. T√™n thu·ªëc: POVIDINE
        Ho·∫°t ch·∫•t: (Ch∆∞a x√°c ƒë·ªãnh)
        H√†m l∆∞·ª£ng: 10% 90ML
        S·ªë l∆∞·ª£ng: 1
        ƒê∆°n v·ªã: Chai

        5. T√™n thu·ªëc: B·ªô chƒÉm s√≥c r·ªën
        Ho·∫°t ch·∫•t: (Ch∆∞a x√°c ƒë·ªãnh)
        H√†m l∆∞·ª£ng: (Ch∆∞a x√°c ƒë·ªãnh)
        S·ªë l∆∞·ª£ng: 1
        ƒê∆°n v·ªã: B·ªô

        ·ªû nh·ªØng thu·ªëc c√≥ d·∫•u (), v√≠ d·ª• nh∆∞ Gliclazid (Staclazide 30 MR) th√¨ to√†n b·ªô ph·∫ßn trong d·∫•u () l√† t√™n thu·ªëc, ph·∫ßn ·ªü ngo√†i l√† ho·∫°t ch·∫•t, nh∆∞ tr∆∞·ªùng h·ª£p n√†y th√¨ t√™n thu·ªëc l√† Staclazide 30 MR, ho·∫°t ch·∫•t l√† Gliclazid. kh√¥ng ƒë∆∞·ª£c ph√©p l·∫•y c·∫£ Gliclazid (Staclazide 30 MR) cho t√™n thu·ªëc.

        C√≤n nh·ªØng tr∆∞·ªùng h·ª£p ch·ªâ c√≥ ch·ªØ in hoa nh∆∞ NEXT G CAL th√¨ NEXT G CAL l√† t√™n thu·ªëc lu√¥n. ch·ªØ O ngay tr∆∞·ªõc ƒë∆°n v·ªã nh∆∞ mg l√† s·ªë 0 b·ªã OCR nh·∫ßm.

        Tuy nhi√™n, n·∫øu thu·ªëc c√≥ () v√† c√≥ ch·ª©a c·∫£ ch·ªØ in hoa to√†n b·ªô, nh∆∞ YESOM 40 40mg (Esomeprazol 40mg) th√¨ ph·∫ßn trong ngo·∫∑c m·ªõi l√† ho·∫°t ch·∫•t, c√≤n ph·∫ßn in hoa to√†n b·ªô lu√¥n lu√¥n l√† t√™n thu·ªëc. Nh∆∞ trong tr∆∞·ªùng h·ª£p n√†y th√¨ YESOM 40 l√† t√™n thu√¥c, Esomeprazol l√† ho·∫°t ch·∫•t.

        ƒê√¥i khi k·∫øt qu·∫£ OCR c≈©ng s·∫Ω b·ªã sai ch√≠nh t·∫£, v√≠ d·ª• 'Cac lon·ªâ vitamin' th√¨ b·∫°n s·ª≠a l·∫°i cho ƒë√∫ng l√† 'C√°c lo·∫°i vitamin'.

        Khi tr·∫£ v·ªÅ S·ªë l∆∞·ª£ng th√¨ ch·ªâ c·∫ßn tr·∫£ v·ªÅ ƒë√∫ng con s·ªë th√¥i, kh√¥ng tr·∫£ v·ªÅ k√®m ƒë∆°n v·ªã t√≠nh, v√≠ d·ª• s·∫Ω tr·∫£ v·ªÅ l√† S·ªë l∆∞·ª£ng: 30 ch·ª© kh√¥ng ƒë∆∞·ª£c tr·∫£ v·ªÅ S·ªë l∆∞·ª£ng: 30 vi√™n.

        Nh·ªõ l·∫•y h√†m l∆∞·ª£ng m·ªôt c√°ch th√¥ng minh ƒë·ªÉ c·ªë g·∫Øng kh√¥ng b·ªã s√≥t h√†m l∆∞·ª£ng ƒë·ªÉ tr·∫£ v·ªÅ nan nh√©.

        B·∫°n hi·ªÉu ch∆∞a, v√† ch·ªâ tr·∫£ l·ªùi b·∫±ng c√°ch li·ªát k√™ ra t√™n thu·ªëc v√† c√°c gi√° tr·ªã t∆∞∆°ng ·ª©ng theo ƒë√∫ng ƒë·ªãnh d·∫°ng m√† v√≠ d·ª• t√¥i ƒë√£ cung c·∫•p. kh√¥ng tr·∫£ l·ªùi b·∫•t c·ª© g√¨ kh√°c. ch·ªâ tr·∫£ l·ªùi ph·∫ßn c·∫ßn thi·∫øt kh√¥ng gi·∫£i th√≠ch b·∫•t c·ª© g√¨ kh√°c, kh√¥ng m·ªü ngo·∫∑c ƒë·ªÅ ch√∫ th√≠ch. ph·∫£i tr·∫£ l·ªùi theo d·∫°ng li·ªát k√™ xu·ªëng d√≤ng nh∆∞ tr√™n. Nh·ªõ l√† ph·∫£i li·ªát k√™ ƒë·ªß s·ªë l∆∞·ª£ng ƒë·∫•y nh√©.
        OCR ƒë√¢y:
        ---
        {ocr_text}
        ---
        """
        response = model.generate_content(prompt, safety_settings=safety_settings)
        return response.text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi API c·ªßa Google AI: {e}")
        return None

def parse_gemini_response(response_text):
    parsed_drugs = []
    blocks = re.split(r'\n*(?=[-\d\.]*\s*T√™n thu·ªëc:)', response_text)
    for block in blocks:
        if not block.strip():
            continue
        ten_thuoc = re.search(r"T√™n thu·ªëc:\s*(.*)", block)
        hoat_chat = re.search(r"Ho·∫°t ch·∫•t:\s*(.*)", block)
        ham_luong = re.search(r"H√†m l∆∞·ª£ng:\s*(.*)", block)
        so_luong = re.search(r"S·ªë l∆∞·ª£ng:\s*(.*)", block)
        don_vi_tinh = re.search(r"ƒê∆°n v·ªã t√≠nh:\s*(.*)", block)
        so_luong_value = np.nan
        if so_luong and "(Ch∆∞a x√°c ƒë·ªãnh)" not in so_luong.group(1):
            so_luong_text = so_luong.group(1).strip()
            number_match = re.match(r'(\d+\.?\d*)', so_luong_text)
            if number_match:
                so_luong_value = number_match.group(1)
        drug_dict = {
            "tenThuoc": ten_thuoc.group(1).strip() if ten_thuoc and "(Ch∆∞a x√°c ƒë·ªãnh)" not in ten_thuoc.group(1) else np.nan,
            "hoatChat": hoat_chat.group(1).strip() if hoat_chat and "(Ch∆∞a x√°c ƒë·ªãnh)" not in hoat_chat.group(1) else np.nan,
            "hamLuong": ham_luong.group(1).strip() if ham_luong and "(Ch∆∞a x√°c ƒë·ªãnh)" not in ham_luong.group(1) else np.nan,
            "soLuong": so_luong_value,
            "donViTinh": don_vi_tinh.group(1).strip() if don_vi_tinh and "(Ch∆∞a x√°c ƒë·ªãnh)" not in don_vi_tinh.group(1) else np.nan,
        }
        if pd.notna(drug_dict["tenThuoc"]) or pd.notna(drug_dict["hoatChat"]):
            parsed_drugs.append(drug_dict)
    return parsed_drugs

st.title("G·ª£i √Ω Gi√° thu·ªëc")

if df_full is not None:
    user_query_text = st.text_input("", placeholder="Nh·∫≠p t√™n thu·ªëc, v√≠ d·ª• t√™n thu·ªëc (ho·∫°t ch·∫•t) h√†m l∆∞·ª£ng s·ªë l∆∞·ª£ng...", label_visibility="collapsed")
    uploaded_file = st.file_uploader("", type=["png", "jpg", "jpeg"], label_visibility="collapsed")

    query_to_process = None
    source = "text"

    if uploaded_file is not None:
        image_bytes = uploaded_file.getvalue()
        st.image(uploaded_file, use_container_width=True)
        with st.spinner("ƒêang ƒë·ªçc ·∫£nh..."):
            ocr_text = " ".join(ocr_reader.readtext(image_bytes, detail=0))
        query_to_process = ocr_text
        source = "ocr"
        with st.expander("Xem to√†n b·ªô vƒÉn b·∫£n nh·∫≠n d·∫°ng ƒë∆∞·ª£c"):
            st.text_area("", ocr_text, height=150)
    elif user_query_text:
        query_to_process = user_query_text

    if query_to_process:
        st.markdown("---")
        with st.spinner("AI ƒëang ph√¢n t√≠ch ƒë∆°n thu·ªëc..."):
            if source == "ocr":
                structured_response = call_gemini_parser(query_to_process)
                if not structured_response:
                    st.error("AI kh√¥ng th·ªÉ ph√¢n t√≠ch vƒÉn b·∫£n t·ª´ ·∫£nh n√†y.")
                    st.stop()
                lines_to_process = parse_gemini_response(structured_response)
            else:
                lines_to_process = [parse_user_query(query_to_process)]

        if not lines_to_process:
            st.warning("Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c ƒë∆°n thu·ªëc n√†o.")
            st.stop()

        valid_drug_count = 0
        total_gia_kk = 0.0
        total_gia_tt = 0.0
        price_details = []

        for i, parsed_info in enumerate(lines_to_process):
            ten_thuoc = parsed_info.get('tenThuoc', '').strip() if pd.notna(parsed_info.get('tenThuoc')) else ''
            hoat_chat = parsed_info.get('hoatChat', '').strip() if pd.notna(parsed_info.get('hoatChat')) else ''
            if not ten_thuoc and not hoat_chat:
                continue

            valid_drug_count += 1
            if len(lines_to_process) > 1:
                st.markdown(f"--- \n ### üíä K·∫øt qu·∫£ cho ƒë∆°n thu·ªëc {valid_drug_count}")

            with st.spinner(f"ƒêang x·ª≠ l√Ω: '{(ten_thuoc or hoat_chat)[:50]}...'"):
                st.markdown(f"**T√™n thu·ªëc:** {parsed_info.get('tenThuoc') or '(Ch∆∞a x√°c ƒë·ªãnh)'}")
                st.markdown(f"**Ho·∫°t ch·∫•t:** {parsed_info.get('hoatChat') or '(Ch∆∞a x√°c ƒë·ªãnh)'}")
                st.markdown(f"**H√†m l∆∞·ª£ng:** {parsed_info.get('hamLuong') or '(Ch∆∞a x√°c ƒë·ªãnh)'}")
                st.markdown(f"**S·ªë l∆∞·ª£ng:** {parsed_info.get('soLuong') or 'N/A'}")
                st.markdown(f"**ƒê∆°n v·ªã t√≠nh:** {parsed_info.get('donViTinh') or 'N/A'}")
                st.markdown("---")

                quantity = float(parsed_info.get('soLuong', '1')) if parsed_info.get('soLuong') and parsed_info.get('soLuong') != 'N/A' else 1.0

                # Step 1: Compare tenThuoc only (>= 95%)
                choices = df_full['tenThuoc'].dropna().tolist()
                best_match, score, _ = process.extractOne(ten_thuoc, choices, scorer=fuzz.ratio) if ten_thuoc else (None, 0, None)

                if best_match and score >= 95:
                    # Direct match based on tenThuoc
                    drug_info_row = df_full[df_full['tenThuoc'] == best_match].iloc[0]
                    st.markdown(f"**Ph∆∞∆°ng th·ª©c:** `Levenshtein Distance (Thu·ªëc: {best_match}, ƒë·ªô t∆∞∆°ng ƒë·ªìng: {score:.0f}%)`")
                    gia_kk = drug_info_row['giaBanBuonDuKien'] * quantity
                    gia_tt = drug_info_row.get('giaThanh', np.nan) * quantity if pd.notna(drug_info_row.get('giaThanh')) else 0.0
                    st.metric("Gi√° K√™ Khai", f"{gia_kk:,.0f} VND" if pd.notna(gia_kk) else "Kh√¥ng c√≥ d·ªØ li·ªáu")
                    st.metric("Gi√° Th·ªã Tr∆∞·ªùng", f"{gia_tt:,.0f} VND" if gia_tt > 0 else "Kh√¥ng c√≥ d·ªØ li·ªáu")
                    price_details.append({
                        'tenThuoc': parsed_info.get('tenThuoc'),
                        'gia_kk': gia_kk if pd.notna(gia_kk) else 0.0,
                        'gia_tt': gia_tt,
                        'quantity': quantity,
                        'donViTinh': parsed_info.get('donViTinh') or 'N/A'
                    })
                    total_gia_kk += gia_kk if pd.notna(gia_kk) else 0.0
                    total_gia_tt += gia_tt
                else:
                    # Step 2: Compare tenThuoc + hoatChat (>= 90%)
                    search_key = f"{ten_thuoc} {hoat_chat}".strip()
                    choices_combined = (df_full['tenThuoc'].fillna('') + ' ' + df_full['hoatChat'].fillna('')).str.strip().tolist()
                    best_match, score, idx = process.extractOne(search_key, choices_combined, scorer=fuzz.ratio) if search_key else (None, 0, None)

                    if best_match and score >= 90:
                        drug_info_row = df_full.iloc[idx]
                        # Check if dosage differs for extrapolation
                        can_extrapolate = False
                        if pd.notna(parsed_info.get('hamLuong')) and pd.notna(drug_info_row.get('hamLuong')):
                            user_dosage_mg = parse_dosage_value(parsed_info.get('hamLuong'))
                            db_dosage_mg = parse_dosage_value(drug_info_row.get('hamLuong'))
                            if user_dosage_mg > 0 and db_dosage_mg > 0 and user_dosage_mg != db_dosage_mg:
                                can_extrapolate = True

                        if can_extrapolate:
                            ratio = user_dosage_mg / db_dosage_mg
                            st.markdown(f"**Ph∆∞∆°ng th·ª©c:** `Extrapolation by Dosage (T·ª∑ l·ªá: {ratio:.2f}x)`")
                            st.caption(f"D·ª±a tr√™n gi√° c·ªßa *{drug_info_row['tenThuoc']}* (ƒë·ªô t∆∞∆°ng ƒë·ªìng: {score:.0f}%)")
                            gia_kk_base = drug_info_row['giaBanBuonDuKien']
                            gia_tt_base = drug_info_row.get('giaThanh', np.nan)
                            gia_kk_extrapolated = gia_kk_base * ratio * quantity
                            gia_tt_extrapolated = gia_tt_base * ratio * quantity if pd.notna(gia_tt_base) else 0.0
                            st.metric("Gi√° K√™ Khai (∆Ø·ªõc t√≠nh)", f"{gia_kk_extrapolated:,.0f} VND" if pd.notna(gia_kk_base) else "Kh√¥ng c√≥ d·ªØ li·ªáu")
                            st.metric("Gi√° Th·ªã Tr∆∞·ªùng (∆Ø·ªõc t√≠nh)", f"{gia_tt_extrapolated:,.0f} VND" if gia_tt_extrapolated > 0 else "Kh√¥ng c√≥ d·ªØ li·ªáu")
                            price_details.append({
                                'tenThuoc': parsed_info.get('tenThuoc'),
                                'gia_kk': gia_kk_extrapolated if pd.notna(gia_kk_base) else 0.0,
                                'gia_tt': gia_tt_extrapolated,
                                'quantity': quantity,
                                'donViTinh': parsed_info.get('donViTinh') or 'N/A'
                            })
                            total_gia_kk += gia_kk_extrapolated if pd.notna(gia_kk_base) else 0.0
                            total_gia_tt += gia_tt_extrapolated
                        else:
                            # Direct match based on tenThuoc + hoatChat
                            st.markdown(f"**Ph∆∞∆°ng th·ª©c:** `Levenshtein Distance (Thu·ªëc: {drug_info_row['tenThuoc']}, ƒë·ªô t∆∞∆°ng ƒë·ªìng: {score:.0f}%)`")
                            gia_kk = drug_info_row['giaBanBuonDuKien'] * quantity
                            gia_tt = drug_info_row.get('giaThanh', np.nan) * quantity if pd.notna(drug_info_row.get('giaThanh')) else 0.0
                            st.metric("Gi√° K√™ Khai", f"{gia_kk:,.0f} VND" if pd.notna(gia_kk) else "Kh√¥ng c√≥ d·ªØ li·ªáu")
                            st.metric("Gi√° Th·ªã Tr∆∞·ªùng", f"{gia_tt:,.0f} VND" if gia_tt > 0 else "Kh√¥ng c√≥ d·ªØ li·ªáu")
                            price_details.append({
                                'tenThuoc': parsed_info.get('tenThuoc'),
                                'gia_kk': gia_kk if pd.notna(gia_kk) else 0.0,
                                'gia_tt': gia_tt,
                                'quantity': quantity,
                                'donViTinh': parsed_info.get('donViTinh') or 'N/A'
                            })
                            total_gia_kk += gia_kk if pd.notna(gia_kk) else 0.0
                            total_gia_tt += gia_tt
                    else:
                        # Use CatBoost Regressor
                        st.markdown(f"**Ph∆∞∆°ng th·ª©c:** `CatBoost Regressor`")
                        st.caption(f"S·ª≠ d·ª•ng th√¥ng tin b·ªï sung (nh√† SX, n∆∞·ªõc SX, D·∫°ng b√†o ch·∫ø...) t·ª´ thu·ªëc t∆∞∆°ng t·ª± nh·∫•t: *{best_match or 'N/A'}*")
                        hybrid_data = {
                            'tenThuoc': parsed_info.get('tenThuoc', 'missing'),
                            'hoatChat': parsed_info.get('hoatChat', 'missing') or (drug_info_row.get('hoatChat', 'missing') if best_match and 'drug_info_row' in locals() else 'missing'),
                            'hamLuong': parsed_info.get('hamLuong', np.nan),
                            'quyCachDongGoi': parsed_info.get('quyCachDongGoi', 'missing'),
                            'doanhNghiepSanXuat': drug_info_row.get('doanhNghiepSanXuat', 'missing') if best_match and 'drug_info_row' in locals() else 'missing',
                            'nuocSanXuat': drug_info_row.get('nuocSanXuat', 'missing') if best_match and 'drug_info_row' in locals() else 'missing',
                            'dangBaoChe': drug_info_row.get('dangBaoChe', 'missing') if best_match and 'drug_info_row' in locals() else 'missing'
                        }
                        try:
                            transformed_data = transform_hybrid_data(hybrid_data, train_cols, categorical_cols, imputer)
                            y_pred_giaThanh_log = model_giaThanh.predict(transformed_data)
                            y_pred_giaBanBuon_log = model_giaBanBuon.predict(transformed_data)
                            gia_tt_pred = scaler_giaThanh.inverse_transform(y_pred_giaThanh_log.reshape(-1, 1)).flatten()[0] * quantity
                            gia_kk_pred = scaler_giaBanBuon.inverse_transform(y_pred_giaBanBuon_log.reshape(-1, 1)).flatten()[0] * quantity
                            st.metric("Gi√° K√™ Khai (D·ª± ƒëo√°n)", f"{gia_kk_pred:,.0f} VND")
                            st.metric("Gi√° Th·ªã Tr∆∞·ªùng (D·ª± ƒëo√°n)", f"{gia_tt_pred:,.0f} VND")
                            price_details.append({
                                'tenThuoc': parsed_info.get('tenThuoc'),
                                'gia_kk': gia_kk_pred,
                                'gia_tt': gia_tt_pred,
                                'quantity': quantity,
                                'donViTinh': parsed_info.get('donViTinh') or 'N/A'
                            })
                            total_gia_kk += gia_kk_pred
                            total_gia_tt += gia_tt_pred
                        except Exception as e:
                            st.error(f"L·ªói khi d·ª± ƒëo√°n: {e}")
                            price_details.append({
                                'tenThuoc': parsed_info.get('tenThuoc'),
                                'gia_kk': 0.0,
                                'gia_tt': 0.0,
                                'quantity': quantity,
                                'donViTinh': parsed_info.get('donViTinh') or 'N/A'
                            })

        if valid_drug_count > 0:
            st.markdown("--- \n ### K·∫øt qu·∫£ cho toa thu·ªëc (d·ª± ƒëo√°n)")
            st.metric("T·ªïng Gi√° K√™ Khai", f"{total_gia_kk:,.0f} VND")
            st.metric("T·ªïng Gi√° Th·ªã Tr∆∞·ªùng", f"{total_gia_tt:,.0f} VND")
            st.markdown("#### Chi ti·∫øt gi√° t·ª´ng thu·ªëc")
            for detail in price_details:
                st.write(f"- **{detail['tenThuoc']}** ({detail['quantity']:.0f} {detail['donViTinh']}): "
                         f"Gi√° K√™ Khai: {detail['gia_kk']:,.0f} VND, "
                         f"Gi√° Th·ªã Tr∆∞·ªùng: {detail['gia_tt']:,.0f} VND")
